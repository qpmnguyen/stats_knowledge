[{"path":"index.html","id":"about-this-book","chapter":"1 About this book","heading":"1 About this book","text":"book statistical concepts basics perhaps complicated models. personal project might contain mistakes. book helps anyway want make corrections, please file GitHub issue book repository.","code":""},{"path":"index.html","id":"planned-chapters","chapter":"1 About this book","heading":"1.1 Planned Chapters","text":"Section 1: Probability concepts\nBasics probability theory\nBasics probability theory\nRandom variables expectations\nRandom variables expectations\nMultiple random variables\nMultiple random variables\nDistributions functions\nDistributions functionsSection 2: Hypothesis testing\nProperties random samples\nPrinciples summary statistics\nPoint interval estimation\nAsymptotic propertiesSection 3: Regression modeling\nBasic linear regression\nBasic linear regression\nGeneralized linear models\nGeneralized linear models\nSurvival models\nSurvival models\nPenalized regression\nPenalized regressionSection 4: Supervised Statistical Learning\nTree models\nTree models\nSupport vector machines\nSupport vector machines\nk Nearest Neighbors\nk Nearest Neighbors","code":""},{"path":"index.html","id":"future-ideas","chapter":"1 About this book","heading":"1.2 Future Ideas","text":"Bayesian statisticsBayesian statisticsMachine Learning (tabular data)Machine Learning (tabular data)","code":""},{"path":"basics-of-probability-theory.html","id":"basics-of-probability-theory","chapter":"2 Basics of probability theory","heading":"2 Basics of probability theory","text":"","code":""},{"path":"basics-of-probability-theory.html","id":"definition-of-probability","chapter":"2 Basics of probability theory","heading":"2.1 Definition of probability","text":"basics probability can drawn set theory. realization experiment often discrete set outcomes, example toss coin two possible outcomes per coin toss either heads tails. probability operations set outcomes quantify outcomes numbers. Let’s define first theorem probability, sample space:Definition 2.1  (Definition sample space) set \\(S\\) possible outcomes particular experiment called sample space experimentFor example coin tossing, \\(S = \\{H, T\\}\\) \\(H\\) heads \\(T\\) tails. experiment performed multiple times, can collect set occurrences \\(T = \\{ H, T, T, H\\}\\) coin tossing experiment coin tossed 4 times. “frequency occurrence” can thought probability.can define probability event \\(\\) occurs simple \\(P()\\) function belief chances \\(\\) occurring. Usually simplest way think probabilities terms frequency occurrences.\\[\nP() = \\frac{\\text{# times can occur}}{\\text{# possible (unique) outcomes sample space}}\n\\]coin tossing example, probability heads single coin toss \\(P(= H) = \\frac{1}{2}\\) two possible options (H T) 1 time H can occur single coin toss.Even though can develop intuition probabilities, end day \\(P()\\) still function. function following axiomatic properties1\\(0 < P() < 1\\)\\(\\sum_i^S P(s_i) = 1\\) \\(s_i \\S\\) \\(S\\) set possible outcomes ()outcomes \\(S\\) must disjoint.","code":""},{"path":"basics-of-probability-theory.html","id":"counting","chapter":"2 Basics of probability theory","heading":"2.2 Counting","text":"Since probabilities finding possible outcomes, counting relevant process. often, counting methods used order construct probability assignments finite sample spaces. lot counting methods can employed boils two features question hand: ) care order objects counted?; b) replacing objects back “bag” counted (replacement). , can construct following 2x2 table., \\(n\\) number objects, \\(r\\) size sample. example ’re drawing 3 marbles bag 10, \\(n = 10\\) \\(r = 3\\)","code":""},{"path":"basics-of-probability-theory.html","id":"conditional-probabilities","chapter":"2 Basics of probability theory","heading":"2.3 Conditional probabilities","text":"properties probabilities ’ve defined prior refer one two events defined respect sample space \\(S\\), say total number possible unique outcomes still denominator probability calculation equation.However, instances events might disjointed, want know conditional probability, probability event occurred given result another event, e.g. \\(P(\\text{ given } B)\\). Intuitively speaking, frequency occurrence formula :\\[\nP(| B) = \\frac{\\text{# events B occur}}{\\text{# events B occur}}\n\\] Essentially ’ve changed denominator possible unique events sample space consider B occurred context ’re operating conditional probability., : \\[\nP(| B) = \\frac{P(\\cap B)}{P(B)}\n\\]","code":""},{"path":"basics-of-probability-theory.html","id":"properties-of-probabilities","chapter":"2 Basics of probability theory","heading":"2.4 Properties of probabilities","text":"using counting methods figure frequency outcomes constructing probabilities events within set possible events, can think combine knowledge set theory construct new probabilities without count everything .","code":""},{"path":"basics-of-probability-theory.html","id":"addition-rules","chapter":"2 Basics of probability theory","heading":"2.4.1 Addition rules","text":"disjoint sets events \\(\\) \\(B\\), can define probability \\(\\) \\(B\\) occurring, e.g. \\(P(\\cup B)\\) \\(P() + P(B)\\). However, \\(\\) \\(B\\) disjoint (.k.\\(\\) \\(B\\) can occur time), generalized additional formula \\(P(\\cup B) = P() + P(B) - P(\\cap B)\\)","code":""},{"path":"basics-of-probability-theory.html","id":"multiplication-rules","chapter":"2 Basics of probability theory","heading":"2.4.2 Multiplication rules","text":"disjoint sets events \\(\\) \\(B\\) can define probability \\(\\) \\(B\\) occuring, e.g. \\(P(\\cap B)\\) \\(P()P(B)\\). \\(\\) \\(B\\) disjoint therefore independent, generalized formula \\(P(\\cap B) = P(|B)P(B)\\)","code":""},{"path":"basics-of-probability-theory.html","id":"bayes-rule","chapter":"2 Basics of probability theory","heading":"2.5 Bayes rule","text":"Bayes rule provide convenient way calculate “switching” conditional probabilities:Theorem 2.1  (Bayes rule) events \\(\\) \\(B\\), can calculate conditional probability \\(P(| B)\\) :\\[\nP(|B) = \\frac{P(B|)P()}{P(B)}\n\\]\\(P()\\) prior probability event occurring \\(P(B)\\) marginal distribution events \\(B\\) occurred (similar frequency occurrence definition conditional probabilities )","code":""},{"path":"random-variables-and-expectations.html","id":"random-variables-and-expectations","chapter":"3 Random variables and expectations","heading":"3 Random variables and expectations","text":"Random variables summary variables allows us think probability without deal original structure. example, ’re polling individuals answers either “Yes” “,” sample space string “Yes” “” long number questions number people. However, can summarize total number “Yes” natural number therefore easier work .random variable therefore function maps sample space \\(S\\) real numbers.","code":""},{"path":"random-variables-and-expectations.html","id":"distribution-functions","chapter":"3 Random variables and expectations","heading":"3.1 Distribution functions","text":"distributions, associate every random variable \\(X\\) cumulative distribution functionDefinition 3.1  (Definition cumulative distribution function) cumulative distribution function cdf random variable \\(X\\) denoted \\(F_X(x)\\) defined \\[\nF_X(x) = P_X(X\\leq x)\n\\]can define whether random variable \\(X\\) continuous based characteristics CDF. CDF continuous, \\(X\\) continuous, CDF step function, \\(X\\) discrete.Definition 3.2  (Identically distributed variables) following two statements equivalentThe random variables \\(X\\) \\(Y\\) identically distributed\\(F_X(x) = F_Y(x)\\) every \\(x\\)addition cumulative distribution function probability density function probability mass function. called “point probabilities”Definition 3.3  (Probability mass function) probability mass function discrete random variable \\(X\\) given \\[\nf_X(x) = P(X = x) \n\\]probability density function continuous random variable \\(X\\) function satisfies\\[\nF_X(x) = \\int_{-\\infty}^{x} f_X(t)\\]","code":""},{"path":"tree-based-methods.html","id":"tree-based-methods","chapter":"4 Tree-based Methods","heading":"4 Tree-based Methods","text":"section heavily adapted ISLR version 22","code":""},{"path":"tree-based-methods.html","id":"basics-of-decision-trees","chapter":"4 Tree-based Methods","heading":"4.1 Basics of decision trees","text":"","code":""},{"path":"tree-based-methods.html","id":"regression-trees","chapter":"4 Tree-based Methods","heading":"4.1.1 Regression trees","text":"Tree-based methods involve stratifying segmenting predictor space number segmented regions. make prediction using mean mode response value training observations region belongs. However, decision trees competitive learning approaches, often users use approaches random forest involves producing multiple trees combined yield single consensus prediction.\nFigure 4.1: Decision boundary division ISLR\nregions \\(R_1\\), \\(R_2\\), \\(R_3\\) terminal nodes leaves tree. specific points split (e.g. Years >= 4.5) internal nodes tree. Essentially, ’re decision tree problem ’re dividing sample space regions \\(R_1,...,R_J\\) minimizes residual sum squares\\[\n\\sum_{= 1}^J \\sum_{\\R_j} (y_i - \\hat{y}_{R_j})^2\n\\]\nSince ’s unreasonable consider every partition, take top-, greedy approach called recursive binary splitting. procedure top-begins top tree (observations belonging region) splits predictor space. procedure greedy step tree-building process, best split made particular step rather looking ahead picking split lead better tree future step.However, decision trees prone overfitting, means low bias large variance since fits data perfectly. usually represented really elaborate “deep trees” (trees lot internal nodes). One way avoid overfitting grow smaller tree growing deep tree prune large tree adding complexity penalty. means calculating tree minimizes following cost function\\[\n\\sum_{m=1}^{|T|} \\sum_{x_i \\R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha|T| \n\\]\nhyperparameter \\(\\alpha\\) controls degree penalty deep trees, higher \\(\\alpha\\) means branches pruned. similar LASSO Ridge penalties linear regression.","code":""},{"path":"tree-based-methods.html","id":"classification-trees","chapter":"4 Tree-based Methods","heading":"4.1.2 Classification Trees","text":"classification tree similar regression tree, except used predict qualitative response rather continuous one. classification tree, predict observation belongs commonly occurring class training observations region belongs. addition class label, ’re interested class proportion among training observations belong region.Instead RSS, evaluate splits based Gini Index, defined :\\[\nG = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk})\n\\]\n\\(\\hat{p}_{mk}\\) proportion training observations \\(m\\)th region \\(k\\)th class. Gini index measure node purity, small value indicates split predominantly one class. alternative Gini index entropy (Shannon’s Entropy)\\[\nD = -\\sum_{k=1}^K \\hat{p}_{mk} \\log \\hat{p}_{mk}\n\\]Similarly, low entropy also indicates purity node.","code":""},{"path":"tree-based-methods.html","id":"comparing-linear-and-tree-models","chapter":"4 Tree-based Methods","heading":"4.1.3 Comparing linear and tree models","text":"can compare form linear regression model\\[\nf(X) = \\beta_0 + \\sum_{j = 1}^p X_j\\beta_j\n\\]\nregression tree\\[\nf(X) = \\sum_{m=1}^M c_m \\cdot 1_{X \\R_m}\n\\]\nissue regression imposes global linear relationship. relationship obeyed, linear regression model usually outperforms decision tree. However, relationship true (e.g. non linear relationship), decision tree usually better classical approach.","code":""},{"path":"tree-based-methods.html","id":"advantages-and-disadvantages-of-trees","chapter":"4 Tree-based Methods","heading":"4.1.4 Advantages and disadvantages of trees","text":"Advantages:Trees easy explain, visualize intuitive.Trees can displayed graphically can handle qualitative predictors withoutt need create dummy variablesDisadvantages:Trees level accuracy regrression approachesTrees robust (small changes data can cause large changes estimated three)","code":""},{"path":"tree-based-methods.html","id":"bagging-boosting-and-bayesian-additive-regression-trees","chapter":"4 Tree-based Methods","heading":"4.2 Bagging, Boosting and Bayesian Additive Regression Trees","text":"ensemble methods builds many small models (.e. weak predictors) combine find single powerful model.","code":""},{"path":"tree-based-methods.html","id":"bagging","chapter":"4 Tree-based Methods","heading":"4.2.1 Bagging","text":"Bagging combine idea bootstrap decision trees. helps solves issue high variance repeatedly creating new data sets average predictions across high variance decision tree.Bag Error Estimation (OOB) approach bagging produces prediction values based OOB evaluation approach rather straight forward average prediction sample \\(B\\) bootstrapped training set. Essentially bootstrap resample \\(b\\), train tree predict values samples sampled \\(b\\). Since boostrap resample \\(b\\) average uses 2/3rds data, sample average \\(B\\)/3 predictions. average predictions together get OOB prediction sample (majority voting case classifications). procedure adopted OOB error can used estimate test error abagged model. OOB MSE classification error computed valid estimate test error.Variable importance measure bagged trees good prediction, removes ease interpretability decision trees . However, method achieve average role predictor record total sum RSS decreased (Gini index) tree split node, averaged \\(B\\) trees. Large value indicates variable important (always chosen across \\(B\\) trees decreased large amount objective function).Random Forest bagged tree model add tweak decorrelate trees. achieved randomly sampling \\(m\\) predictors candidates instead full \\(p\\) number predictors. typically choose \\(m = \\sqrt{p}\\). similarly strong predictor splits, always build similar trees (hence trees correlated). averaging correlated trees, ’re gaining new information resampling procedure (since ’re just getting tree ) lead reduction variance prediction.","code":""},{"path":"tree-based-methods.html","id":"boosting","chapter":"4 Tree-based Methods","heading":"4.2.2 Boosting","text":"Boosting general approach similar bagging can add statistical learning approach. Boosting works similar bagging, instead parallell trees, trees constructed sequentially: tree grown using information previously grown trees (computationally expensive – bagged models can done parallel).Boosting algorithmBy fitting trees residuals, ’re actually adding different model tackle portion relationship captured previous model. Boosted models usually number hyperparameters:number trees \\(B\\). Boosted models can overfit \\(B\\) large process slow. can use cross validation select \\(B\\).shrinkage parameter \\(\\lambda\\). controls rate boosting learns. Typical choices 0.01, 0.001 depends problem. Small \\(\\lambda\\) require large values \\(B\\) good performance.number splits tree, controls complexity boosted ensemble. Often \\(d = 1\\) works well, tree stump one split. \\(d\\) also conisdered interaction depth, controls interaction order boosted model.Usually even stump models really good outperforms high order interaction models random forest","code":""},{"path":"tree-based-methods.html","id":"bayesian-additive-regression-trees","chapter":"4 Tree-based Methods","heading":"4.2.3 Bayesian additive Regression Trees","text":"BART tries capture aspects random forests boosted trees, tree constructed random manner bagging random forests, tree tries capture signal yet accounted current model, boosting.BART modelThis done computing number trees \\(K\\) entire data (similar bagging). However, \\(K\\) trees also grown sequentially iteration \\(B\\) (similar boosting). BART essentially trying bag boosted trees together. difference compared boosting new tree grown residual, computed partial residual\n\\[\nr_i = y_i - \\sum_{k* < k} \\hat{f}^b_{k*}(x_i) - \\sum_{k* > k} \\hat{f}^{b-1}_{k*}(x_i) \n\\]\nfit new tree \\(\\hat{f}_k^b(x)\\) residual \\(r_i\\) perturbing original tree either changes structure tree adding pruning branches change prediction terminal node tree. BART termed Bayesian approach perturbation period similar sampling posterior distribution tree space. algorithm can viewed Markov chain Monte Carlo algorithm. BART models really good already box.","code":""},{"path":"support-vector-machines.html","id":"support-vector-machines","chapter":"5 Support Vector Machines","heading":"5 Support Vector Machines","text":"chapter support vector machines","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
